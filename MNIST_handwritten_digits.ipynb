{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8R/pSy0qnSKLo/phtKJ61",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JustinJiangNext/Handwritten-Digit-Recognizer/blob/main/MNIST_handwritten_digits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QYL7yHph8Bp",
        "outputId": "c8a79f99-bb53-4d49-d819-06291aeba68e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "#@title install libraries\n",
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models\n"
      ],
      "metadata": {
        "id": "PmQ0Q1KNnVsL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define Hyperparameters\n",
        "num_classes = 10\n",
        "num_epochs = 10\n",
        "batch_size = 100\n",
        "lr = 1e-3 #learning rate"
      ],
      "metadata": {
        "id": "buQWJRVqnZMb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Downloading the data and loading into Pytorch DataLoader object\n",
        "\n",
        "train_data = dsets.MNIST(root = './data', train = True,\n",
        "                        transform = transforms.ToTensor(), download = True)\n",
        "\n",
        "test_data = dsets.MNIST(root = './data', train = False,\n",
        "                       transform = transforms.ToTensor())\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  train_data.train_data.cuda()\n",
        "  train_data.train_labels.cuda()\n",
        "  test_data.train_data.cuda()\n",
        "  test_data.train_labels.cuda()\n",
        "\n",
        "train_gen = torch.utils.data.DataLoader(dataset = train_data,\n",
        "                                             batch_size = batch_size,\n",
        "                                             shuffle = True)\n",
        "\n",
        "test_gen = torch.utils.data.DataLoader(dataset = test_data,\n",
        "                                      batch_size = batch_size,\n",
        "                                      shuffle = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dd_P66lnckk",
        "outputId": "2d0ec94f-229b-4aba-8a9d-578140a06c1c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 37.0MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.17MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 8.91MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.09MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define model class\n",
        "\n",
        "class LinearNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LinearNN, self).__init__()\n",
        "    self.fc1 = nn.Linear(28 * 28, 128)\n",
        "    self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.flatten(x, start_dim=1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    x = F.softmax(x, dim=1)\n",
        "    return x\n",
        "\n",
        "class ConvolutionalNN(nn.Module):\n",
        "  def __init__(self):\n",
        "        super(ConvolutionalNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = F.relu(self.conv1(x))\n",
        "      x = self.pool(x)\n",
        "      x = F.relu(self.conv2(x))\n",
        "      x = self.pool(x)\n",
        "      x = torch.flatten(x, start_dim=1)\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = self.fc2(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "class DenseNetNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.densenet = models.densenet121(pretrained=True)\n",
        "        self.densenet.features.conv0 = nn.Conv2d(1, 64, kernel_size = 3, padding = 1)\n",
        "        self.densenet.classifier = nn.Linear(self.densenet.classifier.in_features, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(x.size())\n",
        "        return self.densenet(x)"
      ],
      "metadata": {
        "id": "0uQ-0kbYnfqd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training the model\n",
        "net = LinearNN()\n",
        "if torch.cuda.is_available():\n",
        "  net.cuda()\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "# Training loop\n",
        "net.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for i, (images, labels) in enumerate(train_gen):\n",
        "        if torch.cuda.is_available():\n",
        "          images, labels = images.cuda(), labels.cuda()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = net(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (i+1) % 10 == 0 or (i+1) == len(train_gen):\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_gen)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}] Average Loss: {total_loss/len(train_gen):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dj-e_R2JnjhD",
        "outputId": "47fbfbe8-3c6e-4db3-fe48-3505863cfea9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [10/600], Loss: 2.2322\n",
            "Epoch [1/10], Step [20/600], Loss: 2.1043\n",
            "Epoch [1/10], Step [30/600], Loss: 1.9317\n",
            "Epoch [1/10], Step [40/600], Loss: 1.8606\n",
            "Epoch [1/10], Step [50/600], Loss: 1.7885\n",
            "Epoch [1/10], Step [60/600], Loss: 1.7271\n",
            "Epoch [1/10], Step [70/600], Loss: 1.7053\n",
            "Epoch [1/10], Step [80/600], Loss: 1.6976\n",
            "Epoch [1/10], Step [90/600], Loss: 1.6198\n",
            "Epoch [1/10], Step [100/600], Loss: 1.6244\n",
            "Epoch [1/10], Step [110/600], Loss: 1.6468\n",
            "Epoch [1/10], Step [120/600], Loss: 1.5664\n",
            "Epoch [1/10], Step [130/600], Loss: 1.6231\n",
            "Epoch [1/10], Step [140/600], Loss: 1.6872\n",
            "Epoch [1/10], Step [150/600], Loss: 1.5817\n",
            "Epoch [1/10], Step [160/600], Loss: 1.5995\n",
            "Epoch [1/10], Step [170/600], Loss: 1.6013\n",
            "Epoch [1/10], Step [180/600], Loss: 1.6106\n",
            "Epoch [1/10], Step [190/600], Loss: 1.6337\n",
            "Epoch [1/10], Step [200/600], Loss: 1.5899\n",
            "Epoch [1/10], Step [210/600], Loss: 1.5820\n",
            "Epoch [1/10], Step [220/600], Loss: 1.5830\n",
            "Epoch [1/10], Step [230/600], Loss: 1.5841\n",
            "Epoch [1/10], Step [240/600], Loss: 1.5948\n",
            "Epoch [1/10], Step [250/600], Loss: 1.5801\n",
            "Epoch [1/10], Step [260/600], Loss: 1.6243\n",
            "Epoch [1/10], Step [270/600], Loss: 1.5576\n",
            "Epoch [1/10], Step [280/600], Loss: 1.5699\n",
            "Epoch [1/10], Step [290/600], Loss: 1.5827\n",
            "Epoch [1/10], Step [300/600], Loss: 1.5826\n",
            "Epoch [1/10], Step [310/600], Loss: 1.5584\n",
            "Epoch [1/10], Step [320/600], Loss: 1.5409\n",
            "Epoch [1/10], Step [330/600], Loss: 1.6045\n",
            "Epoch [1/10], Step [340/600], Loss: 1.5599\n",
            "Epoch [1/10], Step [350/600], Loss: 1.5546\n",
            "Epoch [1/10], Step [360/600], Loss: 1.5672\n",
            "Epoch [1/10], Step [370/600], Loss: 1.5274\n",
            "Epoch [1/10], Step [380/600], Loss: 1.5382\n",
            "Epoch [1/10], Step [390/600], Loss: 1.5869\n",
            "Epoch [1/10], Step [400/600], Loss: 1.5630\n",
            "Epoch [1/10], Step [410/600], Loss: 1.5423\n",
            "Epoch [1/10], Step [420/600], Loss: 1.5874\n",
            "Epoch [1/10], Step [430/600], Loss: 1.5408\n",
            "Epoch [1/10], Step [440/600], Loss: 1.5425\n",
            "Epoch [1/10], Step [450/600], Loss: 1.5084\n",
            "Epoch [1/10], Step [460/600], Loss: 1.5474\n",
            "Epoch [1/10], Step [470/600], Loss: 1.5810\n",
            "Epoch [1/10], Step [480/600], Loss: 1.5147\n",
            "Epoch [1/10], Step [490/600], Loss: 1.5696\n",
            "Epoch [1/10], Step [500/600], Loss: 1.5308\n",
            "Epoch [1/10], Step [510/600], Loss: 1.5140\n",
            "Epoch [1/10], Step [520/600], Loss: 1.5585\n",
            "Epoch [1/10], Step [530/600], Loss: 1.5435\n",
            "Epoch [1/10], Step [540/600], Loss: 1.5186\n",
            "Epoch [1/10], Step [550/600], Loss: 1.5603\n",
            "Epoch [1/10], Step [560/600], Loss: 1.5780\n",
            "Epoch [1/10], Step [570/600], Loss: 1.5741\n",
            "Epoch [1/10], Step [580/600], Loss: 1.5507\n",
            "Epoch [1/10], Step [590/600], Loss: 1.5719\n",
            "Epoch [1/10], Step [600/600], Loss: 1.6113\n",
            "Epoch [1] Average Loss: 1.6159\n",
            "Epoch [2/10], Step [10/600], Loss: 1.5297\n",
            "Epoch [2/10], Step [20/600], Loss: 1.5412\n",
            "Epoch [2/10], Step [30/600], Loss: 1.5201\n",
            "Epoch [2/10], Step [40/600], Loss: 1.5062\n",
            "Epoch [2/10], Step [50/600], Loss: 1.5663\n",
            "Epoch [2/10], Step [60/600], Loss: 1.5175\n",
            "Epoch [2/10], Step [70/600], Loss: 1.5747\n",
            "Epoch [2/10], Step [80/600], Loss: 1.5340\n",
            "Epoch [2/10], Step [90/600], Loss: 1.5620\n",
            "Epoch [2/10], Step [100/600], Loss: 1.5417\n",
            "Epoch [2/10], Step [110/600], Loss: 1.5198\n",
            "Epoch [2/10], Step [120/600], Loss: 1.5030\n",
            "Epoch [2/10], Step [130/600], Loss: 1.5232\n",
            "Epoch [2/10], Step [140/600], Loss: 1.5177\n",
            "Epoch [2/10], Step [150/600], Loss: 1.5433\n",
            "Epoch [2/10], Step [160/600], Loss: 1.5425\n",
            "Epoch [2/10], Step [170/600], Loss: 1.5227\n",
            "Epoch [2/10], Step [180/600], Loss: 1.5774\n",
            "Epoch [2/10], Step [190/600], Loss: 1.5009\n",
            "Epoch [2/10], Step [200/600], Loss: 1.5117\n",
            "Epoch [2/10], Step [210/600], Loss: 1.5064\n",
            "Epoch [2/10], Step [220/600], Loss: 1.5312\n",
            "Epoch [2/10], Step [230/600], Loss: 1.5263\n",
            "Epoch [2/10], Step [240/600], Loss: 1.5277\n",
            "Epoch [2/10], Step [250/600], Loss: 1.5253\n",
            "Epoch [2/10], Step [260/600], Loss: 1.5109\n",
            "Epoch [2/10], Step [270/600], Loss: 1.5449\n",
            "Epoch [2/10], Step [280/600], Loss: 1.5264\n",
            "Epoch [2/10], Step [290/600], Loss: 1.5327\n",
            "Epoch [2/10], Step [300/600], Loss: 1.5554\n",
            "Epoch [2/10], Step [310/600], Loss: 1.5549\n",
            "Epoch [2/10], Step [320/600], Loss: 1.4991\n",
            "Epoch [2/10], Step [330/600], Loss: 1.5331\n",
            "Epoch [2/10], Step [340/600], Loss: 1.4971\n",
            "Epoch [2/10], Step [350/600], Loss: 1.5508\n",
            "Epoch [2/10], Step [360/600], Loss: 1.5384\n",
            "Epoch [2/10], Step [370/600], Loss: 1.5251\n",
            "Epoch [2/10], Step [380/600], Loss: 1.5040\n",
            "Epoch [2/10], Step [390/600], Loss: 1.5480\n",
            "Epoch [2/10], Step [400/600], Loss: 1.5254\n",
            "Epoch [2/10], Step [410/600], Loss: 1.5403\n",
            "Epoch [2/10], Step [420/600], Loss: 1.5100\n",
            "Epoch [2/10], Step [430/600], Loss: 1.5744\n",
            "Epoch [2/10], Step [440/600], Loss: 1.5505\n",
            "Epoch [2/10], Step [450/600], Loss: 1.5082\n",
            "Epoch [2/10], Step [460/600], Loss: 1.5317\n",
            "Epoch [2/10], Step [470/600], Loss: 1.5363\n",
            "Epoch [2/10], Step [480/600], Loss: 1.5475\n",
            "Epoch [2/10], Step [490/600], Loss: 1.5532\n",
            "Epoch [2/10], Step [500/600], Loss: 1.5075\n",
            "Epoch [2/10], Step [510/600], Loss: 1.5237\n",
            "Epoch [2/10], Step [520/600], Loss: 1.5422\n",
            "Epoch [2/10], Step [530/600], Loss: 1.5294\n",
            "Epoch [2/10], Step [540/600], Loss: 1.5052\n",
            "Epoch [2/10], Step [550/600], Loss: 1.5408\n",
            "Epoch [2/10], Step [560/600], Loss: 1.5547\n",
            "Epoch [2/10], Step [570/600], Loss: 1.5577\n",
            "Epoch [2/10], Step [580/600], Loss: 1.5283\n",
            "Epoch [2/10], Step [590/600], Loss: 1.5193\n",
            "Epoch [2/10], Step [600/600], Loss: 1.5364\n",
            "Epoch [2] Average Loss: 1.5354\n",
            "Epoch [3/10], Step [10/600], Loss: 1.4966\n",
            "Epoch [3/10], Step [20/600], Loss: 1.5672\n",
            "Epoch [3/10], Step [30/600], Loss: 1.5321\n",
            "Epoch [3/10], Step [40/600], Loss: 1.5812\n",
            "Epoch [3/10], Step [50/600], Loss: 1.4962\n",
            "Epoch [3/10], Step [60/600], Loss: 1.4952\n",
            "Epoch [3/10], Step [70/600], Loss: 1.5157\n",
            "Epoch [3/10], Step [80/600], Loss: 1.5707\n",
            "Epoch [3/10], Step [90/600], Loss: 1.5376\n",
            "Epoch [3/10], Step [100/600], Loss: 1.5224\n",
            "Epoch [3/10], Step [110/600], Loss: 1.5161\n",
            "Epoch [3/10], Step [120/600], Loss: 1.5303\n",
            "Epoch [3/10], Step [130/600], Loss: 1.5442\n",
            "Epoch [3/10], Step [140/600], Loss: 1.5344\n",
            "Epoch [3/10], Step [150/600], Loss: 1.5113\n",
            "Epoch [3/10], Step [160/600], Loss: 1.5515\n",
            "Epoch [3/10], Step [170/600], Loss: 1.5011\n",
            "Epoch [3/10], Step [180/600], Loss: 1.4890\n",
            "Epoch [3/10], Step [190/600], Loss: 1.5908\n",
            "Epoch [3/10], Step [200/600], Loss: 1.5021\n",
            "Epoch [3/10], Step [210/600], Loss: 1.5208\n",
            "Epoch [3/10], Step [220/600], Loss: 1.5048\n",
            "Epoch [3/10], Step [230/600], Loss: 1.4994\n",
            "Epoch [3/10], Step [240/600], Loss: 1.5469\n",
            "Epoch [3/10], Step [250/600], Loss: 1.5168\n",
            "Epoch [3/10], Step [260/600], Loss: 1.4918\n",
            "Epoch [3/10], Step [270/600], Loss: 1.5151\n",
            "Epoch [3/10], Step [280/600], Loss: 1.5348\n",
            "Epoch [3/10], Step [290/600], Loss: 1.5231\n",
            "Epoch [3/10], Step [300/600], Loss: 1.4709\n",
            "Epoch [3/10], Step [310/600], Loss: 1.5591\n",
            "Epoch [3/10], Step [320/600], Loss: 1.5246\n",
            "Epoch [3/10], Step [330/600], Loss: 1.5050\n",
            "Epoch [3/10], Step [340/600], Loss: 1.5232\n",
            "Epoch [3/10], Step [350/600], Loss: 1.5372\n",
            "Epoch [3/10], Step [360/600], Loss: 1.5638\n",
            "Epoch [3/10], Step [370/600], Loss: 1.5596\n",
            "Epoch [3/10], Step [380/600], Loss: 1.5100\n",
            "Epoch [3/10], Step [390/600], Loss: 1.4959\n",
            "Epoch [3/10], Step [400/600], Loss: 1.5151\n",
            "Epoch [3/10], Step [410/600], Loss: 1.5114\n",
            "Epoch [3/10], Step [420/600], Loss: 1.5274\n",
            "Epoch [3/10], Step [430/600], Loss: 1.5034\n",
            "Epoch [3/10], Step [440/600], Loss: 1.5293\n",
            "Epoch [3/10], Step [450/600], Loss: 1.5384\n",
            "Epoch [3/10], Step [460/600], Loss: 1.5088\n",
            "Epoch [3/10], Step [470/600], Loss: 1.5330\n",
            "Epoch [3/10], Step [480/600], Loss: 1.5529\n",
            "Epoch [3/10], Step [490/600], Loss: 1.4871\n",
            "Epoch [3/10], Step [500/600], Loss: 1.5191\n",
            "Epoch [3/10], Step [510/600], Loss: 1.5232\n",
            "Epoch [3/10], Step [520/600], Loss: 1.5378\n",
            "Epoch [3/10], Step [530/600], Loss: 1.5480\n",
            "Epoch [3/10], Step [540/600], Loss: 1.5345\n",
            "Epoch [3/10], Step [550/600], Loss: 1.5123\n",
            "Epoch [3/10], Step [560/600], Loss: 1.5397\n",
            "Epoch [3/10], Step [570/600], Loss: 1.5029\n",
            "Epoch [3/10], Step [580/600], Loss: 1.5158\n",
            "Epoch [3/10], Step [590/600], Loss: 1.4926\n",
            "Epoch [3/10], Step [600/600], Loss: 1.5328\n",
            "Epoch [3] Average Loss: 1.5202\n",
            "Epoch [4/10], Step [10/600], Loss: 1.4872\n",
            "Epoch [4/10], Step [20/600], Loss: 1.5150\n",
            "Epoch [4/10], Step [30/600], Loss: 1.5086\n",
            "Epoch [4/10], Step [40/600], Loss: 1.5185\n",
            "Epoch [4/10], Step [50/600], Loss: 1.5173\n",
            "Epoch [4/10], Step [60/600], Loss: 1.5168\n",
            "Epoch [4/10], Step [70/600], Loss: 1.4969\n",
            "Epoch [4/10], Step [80/600], Loss: 1.5171\n",
            "Epoch [4/10], Step [90/600], Loss: 1.4928\n",
            "Epoch [4/10], Step [100/600], Loss: 1.4837\n",
            "Epoch [4/10], Step [110/600], Loss: 1.5417\n",
            "Epoch [4/10], Step [120/600], Loss: 1.5565\n",
            "Epoch [4/10], Step [130/600], Loss: 1.5007\n",
            "Epoch [4/10], Step [140/600], Loss: 1.5222\n",
            "Epoch [4/10], Step [150/600], Loss: 1.4916\n",
            "Epoch [4/10], Step [160/600], Loss: 1.5198\n",
            "Epoch [4/10], Step [170/600], Loss: 1.4892\n",
            "Epoch [4/10], Step [180/600], Loss: 1.5135\n",
            "Epoch [4/10], Step [190/600], Loss: 1.5159\n",
            "Epoch [4/10], Step [200/600], Loss: 1.5360\n",
            "Epoch [4/10], Step [210/600], Loss: 1.5300\n",
            "Epoch [4/10], Step [220/600], Loss: 1.5004\n",
            "Epoch [4/10], Step [230/600], Loss: 1.5309\n",
            "Epoch [4/10], Step [240/600], Loss: 1.5270\n",
            "Epoch [4/10], Step [250/600], Loss: 1.5064\n",
            "Epoch [4/10], Step [260/600], Loss: 1.5469\n",
            "Epoch [4/10], Step [270/600], Loss: 1.4970\n",
            "Epoch [4/10], Step [280/600], Loss: 1.5053\n",
            "Epoch [4/10], Step [290/600], Loss: 1.4939\n",
            "Epoch [4/10], Step [300/600], Loss: 1.4898\n",
            "Epoch [4/10], Step [310/600], Loss: 1.4893\n",
            "Epoch [4/10], Step [320/600], Loss: 1.5073\n",
            "Epoch [4/10], Step [330/600], Loss: 1.5325\n",
            "Epoch [4/10], Step [340/600], Loss: 1.4906\n",
            "Epoch [4/10], Step [350/600], Loss: 1.5119\n",
            "Epoch [4/10], Step [360/600], Loss: 1.5012\n",
            "Epoch [4/10], Step [370/600], Loss: 1.4949\n",
            "Epoch [4/10], Step [380/600], Loss: 1.5530\n",
            "Epoch [4/10], Step [390/600], Loss: 1.5079\n",
            "Epoch [4/10], Step [400/600], Loss: 1.5167\n",
            "Epoch [4/10], Step [410/600], Loss: 1.4974\n",
            "Epoch [4/10], Step [420/600], Loss: 1.5302\n",
            "Epoch [4/10], Step [430/600], Loss: 1.5034\n",
            "Epoch [4/10], Step [440/600], Loss: 1.4963\n",
            "Epoch [4/10], Step [450/600], Loss: 1.5189\n",
            "Epoch [4/10], Step [460/600], Loss: 1.5111\n",
            "Epoch [4/10], Step [470/600], Loss: 1.5141\n",
            "Epoch [4/10], Step [480/600], Loss: 1.4832\n",
            "Epoch [4/10], Step [490/600], Loss: 1.5207\n",
            "Epoch [4/10], Step [500/600], Loss: 1.5347\n",
            "Epoch [4/10], Step [510/600], Loss: 1.5073\n",
            "Epoch [4/10], Step [520/600], Loss: 1.5146\n",
            "Epoch [4/10], Step [530/600], Loss: 1.4856\n",
            "Epoch [4/10], Step [540/600], Loss: 1.5038\n",
            "Epoch [4/10], Step [550/600], Loss: 1.4928\n",
            "Epoch [4/10], Step [560/600], Loss: 1.5007\n",
            "Epoch [4/10], Step [570/600], Loss: 1.4978\n",
            "Epoch [4/10], Step [580/600], Loss: 1.4944\n",
            "Epoch [4/10], Step [590/600], Loss: 1.5135\n",
            "Epoch [4/10], Step [600/600], Loss: 1.5063\n",
            "Epoch [4] Average Loss: 1.5109\n",
            "Epoch [5/10], Step [10/600], Loss: 1.4926\n",
            "Epoch [5/10], Step [20/600], Loss: 1.5211\n",
            "Epoch [5/10], Step [30/600], Loss: 1.4832\n",
            "Epoch [5/10], Step [40/600], Loss: 1.4798\n",
            "Epoch [5/10], Step [50/600], Loss: 1.4767\n",
            "Epoch [5/10], Step [60/600], Loss: 1.4881\n",
            "Epoch [5/10], Step [70/600], Loss: 1.5111\n",
            "Epoch [5/10], Step [80/600], Loss: 1.5060\n",
            "Epoch [5/10], Step [90/600], Loss: 1.5093\n",
            "Epoch [5/10], Step [100/600], Loss: 1.5146\n",
            "Epoch [5/10], Step [110/600], Loss: 1.5081\n",
            "Epoch [5/10], Step [120/600], Loss: 1.5540\n",
            "Epoch [5/10], Step [130/600], Loss: 1.5040\n",
            "Epoch [5/10], Step [140/600], Loss: 1.4804\n",
            "Epoch [5/10], Step [150/600], Loss: 1.5234\n",
            "Epoch [5/10], Step [160/600], Loss: 1.5208\n",
            "Epoch [5/10], Step [170/600], Loss: 1.5083\n",
            "Epoch [5/10], Step [180/600], Loss: 1.5065\n",
            "Epoch [5/10], Step [190/600], Loss: 1.4833\n",
            "Epoch [5/10], Step [200/600], Loss: 1.4927\n",
            "Epoch [5/10], Step [210/600], Loss: 1.4932\n",
            "Epoch [5/10], Step [220/600], Loss: 1.5237\n",
            "Epoch [5/10], Step [230/600], Loss: 1.4881\n",
            "Epoch [5/10], Step [240/600], Loss: 1.4877\n",
            "Epoch [5/10], Step [250/600], Loss: 1.5157\n",
            "Epoch [5/10], Step [260/600], Loss: 1.5098\n",
            "Epoch [5/10], Step [270/600], Loss: 1.4907\n",
            "Epoch [5/10], Step [280/600], Loss: 1.5551\n",
            "Epoch [5/10], Step [290/600], Loss: 1.5013\n",
            "Epoch [5/10], Step [300/600], Loss: 1.5085\n",
            "Epoch [5/10], Step [310/600], Loss: 1.5085\n",
            "Epoch [5/10], Step [320/600], Loss: 1.5102\n",
            "Epoch [5/10], Step [330/600], Loss: 1.4755\n",
            "Epoch [5/10], Step [340/600], Loss: 1.4883\n",
            "Epoch [5/10], Step [350/600], Loss: 1.4871\n",
            "Epoch [5/10], Step [360/600], Loss: 1.5098\n",
            "Epoch [5/10], Step [370/600], Loss: 1.4728\n",
            "Epoch [5/10], Step [380/600], Loss: 1.5262\n",
            "Epoch [5/10], Step [390/600], Loss: 1.4945\n",
            "Epoch [5/10], Step [400/600], Loss: 1.5223\n",
            "Epoch [5/10], Step [410/600], Loss: 1.4912\n",
            "Epoch [5/10], Step [420/600], Loss: 1.5193\n",
            "Epoch [5/10], Step [430/600], Loss: 1.4956\n",
            "Epoch [5/10], Step [440/600], Loss: 1.4888\n",
            "Epoch [5/10], Step [450/600], Loss: 1.5218\n",
            "Epoch [5/10], Step [460/600], Loss: 1.5074\n",
            "Epoch [5/10], Step [470/600], Loss: 1.5110\n",
            "Epoch [5/10], Step [480/600], Loss: 1.5200\n",
            "Epoch [5/10], Step [490/600], Loss: 1.4969\n",
            "Epoch [5/10], Step [500/600], Loss: 1.4992\n",
            "Epoch [5/10], Step [510/600], Loss: 1.5360\n",
            "Epoch [5/10], Step [520/600], Loss: 1.4746\n",
            "Epoch [5/10], Step [530/600], Loss: 1.5318\n",
            "Epoch [5/10], Step [540/600], Loss: 1.5027\n",
            "Epoch [5/10], Step [550/600], Loss: 1.4912\n",
            "Epoch [5/10], Step [560/600], Loss: 1.4821\n",
            "Epoch [5/10], Step [570/600], Loss: 1.4817\n",
            "Epoch [5/10], Step [580/600], Loss: 1.4950\n",
            "Epoch [5/10], Step [590/600], Loss: 1.4801\n",
            "Epoch [5/10], Step [600/600], Loss: 1.5110\n",
            "Epoch [5] Average Loss: 1.5044\n",
            "Epoch [6/10], Step [10/600], Loss: 1.5310\n",
            "Epoch [6/10], Step [20/600], Loss: 1.4926\n",
            "Epoch [6/10], Step [30/600], Loss: 1.4942\n",
            "Epoch [6/10], Step [40/600], Loss: 1.5771\n",
            "Epoch [6/10], Step [50/600], Loss: 1.4801\n",
            "Epoch [6/10], Step [60/600], Loss: 1.5201\n",
            "Epoch [6/10], Step [70/600], Loss: 1.4784\n",
            "Epoch [6/10], Step [80/600], Loss: 1.4899\n",
            "Epoch [6/10], Step [90/600], Loss: 1.4860\n",
            "Epoch [6/10], Step [100/600], Loss: 1.4800\n",
            "Epoch [6/10], Step [110/600], Loss: 1.4999\n",
            "Epoch [6/10], Step [120/600], Loss: 1.5253\n",
            "Epoch [6/10], Step [130/600], Loss: 1.4946\n",
            "Epoch [6/10], Step [140/600], Loss: 1.4826\n",
            "Epoch [6/10], Step [150/600], Loss: 1.4922\n",
            "Epoch [6/10], Step [160/600], Loss: 1.4902\n",
            "Epoch [6/10], Step [170/600], Loss: 1.5182\n",
            "Epoch [6/10], Step [180/600], Loss: 1.4996\n",
            "Epoch [6/10], Step [190/600], Loss: 1.4646\n",
            "Epoch [6/10], Step [200/600], Loss: 1.4913\n",
            "Epoch [6/10], Step [210/600], Loss: 1.4897\n",
            "Epoch [6/10], Step [220/600], Loss: 1.5146\n",
            "Epoch [6/10], Step [230/600], Loss: 1.4820\n",
            "Epoch [6/10], Step [240/600], Loss: 1.5012\n",
            "Epoch [6/10], Step [250/600], Loss: 1.4813\n",
            "Epoch [6/10], Step [260/600], Loss: 1.5330\n",
            "Epoch [6/10], Step [270/600], Loss: 1.4828\n",
            "Epoch [6/10], Step [280/600], Loss: 1.4873\n",
            "Epoch [6/10], Step [290/600], Loss: 1.4876\n",
            "Epoch [6/10], Step [300/600], Loss: 1.5009\n",
            "Epoch [6/10], Step [310/600], Loss: 1.5109\n",
            "Epoch [6/10], Step [320/600], Loss: 1.5166\n",
            "Epoch [6/10], Step [330/600], Loss: 1.4892\n",
            "Epoch [6/10], Step [340/600], Loss: 1.4987\n",
            "Epoch [6/10], Step [350/600], Loss: 1.5017\n",
            "Epoch [6/10], Step [360/600], Loss: 1.5016\n",
            "Epoch [6/10], Step [370/600], Loss: 1.5127\n",
            "Epoch [6/10], Step [380/600], Loss: 1.5159\n",
            "Epoch [6/10], Step [390/600], Loss: 1.4916\n",
            "Epoch [6/10], Step [400/600], Loss: 1.5271\n",
            "Epoch [6/10], Step [410/600], Loss: 1.5049\n",
            "Epoch [6/10], Step [420/600], Loss: 1.4925\n",
            "Epoch [6/10], Step [430/600], Loss: 1.5003\n",
            "Epoch [6/10], Step [440/600], Loss: 1.4675\n",
            "Epoch [6/10], Step [450/600], Loss: 1.4988\n",
            "Epoch [6/10], Step [460/600], Loss: 1.4928\n",
            "Epoch [6/10], Step [470/600], Loss: 1.4786\n",
            "Epoch [6/10], Step [480/600], Loss: 1.4707\n",
            "Epoch [6/10], Step [490/600], Loss: 1.5115\n",
            "Epoch [6/10], Step [500/600], Loss: 1.5000\n",
            "Epoch [6/10], Step [510/600], Loss: 1.5261\n",
            "Epoch [6/10], Step [520/600], Loss: 1.5049\n",
            "Epoch [6/10], Step [530/600], Loss: 1.5392\n",
            "Epoch [6/10], Step [540/600], Loss: 1.5093\n",
            "Epoch [6/10], Step [550/600], Loss: 1.4822\n",
            "Epoch [6/10], Step [560/600], Loss: 1.5170\n",
            "Epoch [6/10], Step [570/600], Loss: 1.4675\n",
            "Epoch [6/10], Step [580/600], Loss: 1.4852\n",
            "Epoch [6/10], Step [590/600], Loss: 1.5006\n",
            "Epoch [6/10], Step [600/600], Loss: 1.5000\n",
            "Epoch [6] Average Loss: 1.4992\n",
            "Epoch [7/10], Step [10/600], Loss: 1.4879\n",
            "Epoch [7/10], Step [20/600], Loss: 1.4900\n",
            "Epoch [7/10], Step [30/600], Loss: 1.5206\n",
            "Epoch [7/10], Step [40/600], Loss: 1.4873\n",
            "Epoch [7/10], Step [50/600], Loss: 1.5012\n",
            "Epoch [7/10], Step [60/600], Loss: 1.4922\n",
            "Epoch [7/10], Step [70/600], Loss: 1.4829\n",
            "Epoch [7/10], Step [80/600], Loss: 1.4852\n",
            "Epoch [7/10], Step [90/600], Loss: 1.5241\n",
            "Epoch [7/10], Step [100/600], Loss: 1.5159\n",
            "Epoch [7/10], Step [110/600], Loss: 1.5164\n",
            "Epoch [7/10], Step [120/600], Loss: 1.4872\n",
            "Epoch [7/10], Step [130/600], Loss: 1.4971\n",
            "Epoch [7/10], Step [140/600], Loss: 1.5406\n",
            "Epoch [7/10], Step [150/600], Loss: 1.4753\n",
            "Epoch [7/10], Step [160/600], Loss: 1.4874\n",
            "Epoch [7/10], Step [170/600], Loss: 1.4798\n",
            "Epoch [7/10], Step [180/600], Loss: 1.4768\n",
            "Epoch [7/10], Step [190/600], Loss: 1.4934\n",
            "Epoch [7/10], Step [200/600], Loss: 1.4765\n",
            "Epoch [7/10], Step [210/600], Loss: 1.4662\n",
            "Epoch [7/10], Step [220/600], Loss: 1.5042\n",
            "Epoch [7/10], Step [230/600], Loss: 1.4997\n",
            "Epoch [7/10], Step [240/600], Loss: 1.5252\n",
            "Epoch [7/10], Step [250/600], Loss: 1.4918\n",
            "Epoch [7/10], Step [260/600], Loss: 1.4783\n",
            "Epoch [7/10], Step [270/600], Loss: 1.4720\n",
            "Epoch [7/10], Step [280/600], Loss: 1.4996\n",
            "Epoch [7/10], Step [290/600], Loss: 1.4986\n",
            "Epoch [7/10], Step [300/600], Loss: 1.5011\n",
            "Epoch [7/10], Step [310/600], Loss: 1.4915\n",
            "Epoch [7/10], Step [320/600], Loss: 1.4819\n",
            "Epoch [7/10], Step [330/600], Loss: 1.4900\n",
            "Epoch [7/10], Step [340/600], Loss: 1.4979\n",
            "Epoch [7/10], Step [350/600], Loss: 1.4805\n",
            "Epoch [7/10], Step [360/600], Loss: 1.5058\n",
            "Epoch [7/10], Step [370/600], Loss: 1.5122\n",
            "Epoch [7/10], Step [380/600], Loss: 1.4803\n",
            "Epoch [7/10], Step [390/600], Loss: 1.4803\n",
            "Epoch [7/10], Step [400/600], Loss: 1.4969\n",
            "Epoch [7/10], Step [410/600], Loss: 1.4881\n",
            "Epoch [7/10], Step [420/600], Loss: 1.4970\n",
            "Epoch [7/10], Step [430/600], Loss: 1.5074\n",
            "Epoch [7/10], Step [440/600], Loss: 1.4952\n",
            "Epoch [7/10], Step [450/600], Loss: 1.5091\n",
            "Epoch [7/10], Step [460/600], Loss: 1.5062\n",
            "Epoch [7/10], Step [470/600], Loss: 1.4888\n",
            "Epoch [7/10], Step [480/600], Loss: 1.4871\n",
            "Epoch [7/10], Step [490/600], Loss: 1.4721\n",
            "Epoch [7/10], Step [500/600], Loss: 1.4936\n",
            "Epoch [7/10], Step [510/600], Loss: 1.4881\n",
            "Epoch [7/10], Step [520/600], Loss: 1.4795\n",
            "Epoch [7/10], Step [530/600], Loss: 1.5046\n",
            "Epoch [7/10], Step [540/600], Loss: 1.4772\n",
            "Epoch [7/10], Step [550/600], Loss: 1.4923\n",
            "Epoch [7/10], Step [560/600], Loss: 1.4762\n",
            "Epoch [7/10], Step [570/600], Loss: 1.4994\n",
            "Epoch [7/10], Step [580/600], Loss: 1.5075\n",
            "Epoch [7/10], Step [590/600], Loss: 1.4754\n",
            "Epoch [7/10], Step [600/600], Loss: 1.5086\n",
            "Epoch [7] Average Loss: 1.4948\n",
            "Epoch [8/10], Step [10/600], Loss: 1.4891\n",
            "Epoch [8/10], Step [20/600], Loss: 1.4749\n",
            "Epoch [8/10], Step [30/600], Loss: 1.4789\n",
            "Epoch [8/10], Step [40/600], Loss: 1.4879\n",
            "Epoch [8/10], Step [50/600], Loss: 1.4963\n",
            "Epoch [8/10], Step [60/600], Loss: 1.4866\n",
            "Epoch [8/10], Step [70/600], Loss: 1.4714\n",
            "Epoch [8/10], Step [80/600], Loss: 1.4728\n",
            "Epoch [8/10], Step [90/600], Loss: 1.5023\n",
            "Epoch [8/10], Step [100/600], Loss: 1.4822\n",
            "Epoch [8/10], Step [110/600], Loss: 1.4668\n",
            "Epoch [8/10], Step [120/600], Loss: 1.4879\n",
            "Epoch [8/10], Step [130/600], Loss: 1.5072\n",
            "Epoch [8/10], Step [140/600], Loss: 1.4676\n",
            "Epoch [8/10], Step [150/600], Loss: 1.4887\n",
            "Epoch [8/10], Step [160/600], Loss: 1.4814\n",
            "Epoch [8/10], Step [170/600], Loss: 1.5150\n",
            "Epoch [8/10], Step [180/600], Loss: 1.4847\n",
            "Epoch [8/10], Step [190/600], Loss: 1.4758\n",
            "Epoch [8/10], Step [200/600], Loss: 1.5048\n",
            "Epoch [8/10], Step [210/600], Loss: 1.5035\n",
            "Epoch [8/10], Step [220/600], Loss: 1.4997\n",
            "Epoch [8/10], Step [230/600], Loss: 1.4905\n",
            "Epoch [8/10], Step [240/600], Loss: 1.4968\n",
            "Epoch [8/10], Step [250/600], Loss: 1.5003\n",
            "Epoch [8/10], Step [260/600], Loss: 1.4754\n",
            "Epoch [8/10], Step [270/600], Loss: 1.4947\n",
            "Epoch [8/10], Step [280/600], Loss: 1.4917\n",
            "Epoch [8/10], Step [290/600], Loss: 1.4948\n",
            "Epoch [8/10], Step [300/600], Loss: 1.5131\n",
            "Epoch [8/10], Step [310/600], Loss: 1.4815\n",
            "Epoch [8/10], Step [320/600], Loss: 1.5229\n",
            "Epoch [8/10], Step [330/600], Loss: 1.4935\n",
            "Epoch [8/10], Step [340/600], Loss: 1.5041\n",
            "Epoch [8/10], Step [350/600], Loss: 1.4857\n",
            "Epoch [8/10], Step [360/600], Loss: 1.4775\n",
            "Epoch [8/10], Step [370/600], Loss: 1.4876\n",
            "Epoch [8/10], Step [380/600], Loss: 1.4781\n",
            "Epoch [8/10], Step [390/600], Loss: 1.5012\n",
            "Epoch [8/10], Step [400/600], Loss: 1.4893\n",
            "Epoch [8/10], Step [410/600], Loss: 1.4986\n",
            "Epoch [8/10], Step [420/600], Loss: 1.4790\n",
            "Epoch [8/10], Step [430/600], Loss: 1.5053\n",
            "Epoch [8/10], Step [440/600], Loss: 1.5070\n",
            "Epoch [8/10], Step [450/600], Loss: 1.4696\n",
            "Epoch [8/10], Step [460/600], Loss: 1.4963\n",
            "Epoch [8/10], Step [470/600], Loss: 1.4704\n",
            "Epoch [8/10], Step [480/600], Loss: 1.4883\n",
            "Epoch [8/10], Step [490/600], Loss: 1.4752\n",
            "Epoch [8/10], Step [500/600], Loss: 1.4964\n",
            "Epoch [8/10], Step [510/600], Loss: 1.4807\n",
            "Epoch [8/10], Step [520/600], Loss: 1.4635\n",
            "Epoch [8/10], Step [530/600], Loss: 1.4901\n",
            "Epoch [8/10], Step [540/600], Loss: 1.4788\n",
            "Epoch [8/10], Step [550/600], Loss: 1.4841\n",
            "Epoch [8/10], Step [560/600], Loss: 1.4816\n",
            "Epoch [8/10], Step [570/600], Loss: 1.5037\n",
            "Epoch [8/10], Step [580/600], Loss: 1.4863\n",
            "Epoch [8/10], Step [590/600], Loss: 1.4788\n",
            "Epoch [8/10], Step [600/600], Loss: 1.4836\n",
            "Epoch [8] Average Loss: 1.4911\n",
            "Epoch [9/10], Step [10/600], Loss: 1.4862\n",
            "Epoch [9/10], Step [20/600], Loss: 1.4937\n",
            "Epoch [9/10], Step [30/600], Loss: 1.5014\n",
            "Epoch [9/10], Step [40/600], Loss: 1.4820\n",
            "Epoch [9/10], Step [50/600], Loss: 1.5025\n",
            "Epoch [9/10], Step [60/600], Loss: 1.4797\n",
            "Epoch [9/10], Step [70/600], Loss: 1.4836\n",
            "Epoch [9/10], Step [80/600], Loss: 1.4923\n",
            "Epoch [9/10], Step [90/600], Loss: 1.4836\n",
            "Epoch [9/10], Step [100/600], Loss: 1.4786\n",
            "Epoch [9/10], Step [110/600], Loss: 1.4714\n",
            "Epoch [9/10], Step [120/600], Loss: 1.4624\n",
            "Epoch [9/10], Step [130/600], Loss: 1.5025\n",
            "Epoch [9/10], Step [140/600], Loss: 1.4876\n",
            "Epoch [9/10], Step [150/600], Loss: 1.5045\n",
            "Epoch [9/10], Step [160/600], Loss: 1.4872\n",
            "Epoch [9/10], Step [170/600], Loss: 1.4950\n",
            "Epoch [9/10], Step [180/600], Loss: 1.4855\n",
            "Epoch [9/10], Step [190/600], Loss: 1.4683\n",
            "Epoch [9/10], Step [200/600], Loss: 1.4819\n",
            "Epoch [9/10], Step [210/600], Loss: 1.4720\n",
            "Epoch [9/10], Step [220/600], Loss: 1.4843\n",
            "Epoch [9/10], Step [230/600], Loss: 1.4787\n",
            "Epoch [9/10], Step [240/600], Loss: 1.4781\n",
            "Epoch [9/10], Step [250/600], Loss: 1.4974\n",
            "Epoch [9/10], Step [260/600], Loss: 1.4862\n",
            "Epoch [9/10], Step [270/600], Loss: 1.5084\n",
            "Epoch [9/10], Step [280/600], Loss: 1.4969\n",
            "Epoch [9/10], Step [290/600], Loss: 1.4947\n",
            "Epoch [9/10], Step [300/600], Loss: 1.4626\n",
            "Epoch [9/10], Step [310/600], Loss: 1.4963\n",
            "Epoch [9/10], Step [320/600], Loss: 1.5069\n",
            "Epoch [9/10], Step [330/600], Loss: 1.5076\n",
            "Epoch [9/10], Step [340/600], Loss: 1.4940\n",
            "Epoch [9/10], Step [350/600], Loss: 1.5188\n",
            "Epoch [9/10], Step [360/600], Loss: 1.4888\n",
            "Epoch [9/10], Step [370/600], Loss: 1.5082\n",
            "Epoch [9/10], Step [380/600], Loss: 1.4887\n",
            "Epoch [9/10], Step [390/600], Loss: 1.4835\n",
            "Epoch [9/10], Step [400/600], Loss: 1.4754\n",
            "Epoch [9/10], Step [410/600], Loss: 1.4749\n",
            "Epoch [9/10], Step [420/600], Loss: 1.4895\n",
            "Epoch [9/10], Step [430/600], Loss: 1.4943\n",
            "Epoch [9/10], Step [440/600], Loss: 1.4936\n",
            "Epoch [9/10], Step [450/600], Loss: 1.5105\n",
            "Epoch [9/10], Step [460/600], Loss: 1.4772\n",
            "Epoch [9/10], Step [470/600], Loss: 1.4775\n",
            "Epoch [9/10], Step [480/600], Loss: 1.4746\n",
            "Epoch [9/10], Step [490/600], Loss: 1.4806\n",
            "Epoch [9/10], Step [500/600], Loss: 1.5244\n",
            "Epoch [9/10], Step [510/600], Loss: 1.4875\n",
            "Epoch [9/10], Step [520/600], Loss: 1.5115\n",
            "Epoch [9/10], Step [530/600], Loss: 1.4834\n",
            "Epoch [9/10], Step [540/600], Loss: 1.4883\n",
            "Epoch [9/10], Step [550/600], Loss: 1.4903\n",
            "Epoch [9/10], Step [560/600], Loss: 1.4764\n",
            "Epoch [9/10], Step [570/600], Loss: 1.4899\n",
            "Epoch [9/10], Step [580/600], Loss: 1.4910\n",
            "Epoch [9/10], Step [590/600], Loss: 1.4812\n",
            "Epoch [9/10], Step [600/600], Loss: 1.5141\n",
            "Epoch [9] Average Loss: 1.4882\n",
            "Epoch [10/10], Step [10/600], Loss: 1.4648\n",
            "Epoch [10/10], Step [20/600], Loss: 1.5033\n",
            "Epoch [10/10], Step [30/600], Loss: 1.4757\n",
            "Epoch [10/10], Step [40/600], Loss: 1.4720\n",
            "Epoch [10/10], Step [50/600], Loss: 1.4925\n",
            "Epoch [10/10], Step [60/600], Loss: 1.5155\n",
            "Epoch [10/10], Step [70/600], Loss: 1.4909\n",
            "Epoch [10/10], Step [80/600], Loss: 1.4806\n",
            "Epoch [10/10], Step [90/600], Loss: 1.4847\n",
            "Epoch [10/10], Step [100/600], Loss: 1.4935\n",
            "Epoch [10/10], Step [110/600], Loss: 1.4834\n",
            "Epoch [10/10], Step [120/600], Loss: 1.4661\n",
            "Epoch [10/10], Step [130/600], Loss: 1.4825\n",
            "Epoch [10/10], Step [140/600], Loss: 1.4704\n",
            "Epoch [10/10], Step [150/600], Loss: 1.4730\n",
            "Epoch [10/10], Step [160/600], Loss: 1.4927\n",
            "Epoch [10/10], Step [170/600], Loss: 1.4909\n",
            "Epoch [10/10], Step [180/600], Loss: 1.4912\n",
            "Epoch [10/10], Step [190/600], Loss: 1.4652\n",
            "Epoch [10/10], Step [200/600], Loss: 1.4933\n",
            "Epoch [10/10], Step [210/600], Loss: 1.4829\n",
            "Epoch [10/10], Step [220/600], Loss: 1.4765\n",
            "Epoch [10/10], Step [230/600], Loss: 1.4830\n",
            "Epoch [10/10], Step [240/600], Loss: 1.4901\n",
            "Epoch [10/10], Step [250/600], Loss: 1.4956\n",
            "Epoch [10/10], Step [260/600], Loss: 1.5125\n",
            "Epoch [10/10], Step [270/600], Loss: 1.4713\n",
            "Epoch [10/10], Step [280/600], Loss: 1.4881\n",
            "Epoch [10/10], Step [290/600], Loss: 1.4745\n",
            "Epoch [10/10], Step [300/600], Loss: 1.5187\n",
            "Epoch [10/10], Step [310/600], Loss: 1.4850\n",
            "Epoch [10/10], Step [320/600], Loss: 1.4906\n",
            "Epoch [10/10], Step [330/600], Loss: 1.4946\n",
            "Epoch [10/10], Step [340/600], Loss: 1.4718\n",
            "Epoch [10/10], Step [350/600], Loss: 1.4948\n",
            "Epoch [10/10], Step [360/600], Loss: 1.4757\n",
            "Epoch [10/10], Step [370/600], Loss: 1.4731\n",
            "Epoch [10/10], Step [380/600], Loss: 1.5045\n",
            "Epoch [10/10], Step [390/600], Loss: 1.5088\n",
            "Epoch [10/10], Step [400/600], Loss: 1.4980\n",
            "Epoch [10/10], Step [410/600], Loss: 1.4904\n",
            "Epoch [10/10], Step [420/600], Loss: 1.5028\n",
            "Epoch [10/10], Step [430/600], Loss: 1.4744\n",
            "Epoch [10/10], Step [440/600], Loss: 1.4688\n",
            "Epoch [10/10], Step [450/600], Loss: 1.4744\n",
            "Epoch [10/10], Step [460/600], Loss: 1.5032\n",
            "Epoch [10/10], Step [470/600], Loss: 1.4801\n",
            "Epoch [10/10], Step [480/600], Loss: 1.4810\n",
            "Epoch [10/10], Step [490/600], Loss: 1.4844\n",
            "Epoch [10/10], Step [500/600], Loss: 1.4889\n",
            "Epoch [10/10], Step [510/600], Loss: 1.5267\n",
            "Epoch [10/10], Step [520/600], Loss: 1.4747\n",
            "Epoch [10/10], Step [530/600], Loss: 1.4947\n",
            "Epoch [10/10], Step [540/600], Loss: 1.4937\n",
            "Epoch [10/10], Step [550/600], Loss: 1.4732\n",
            "Epoch [10/10], Step [560/600], Loss: 1.4943\n",
            "Epoch [10/10], Step [570/600], Loss: 1.4769\n",
            "Epoch [10/10], Step [580/600], Loss: 1.4721\n",
            "Epoch [10/10], Step [590/600], Loss: 1.4965\n",
            "Epoch [10/10], Step [600/600], Loss: 1.4884\n",
            "Epoch [10] Average Loss: 1.4854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Evaluating Accuracy\n",
        "net.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_gen:\n",
        "        if torch.cuda.is_available():\n",
        "          images, labels = images.cuda(), labels.cuda()\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy on test data: {100 * correct / total:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzOEqSEinnFa",
        "outputId": "17a87ea3-5a80-4975-94bf-e5f1d4847199"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test data: 96.92%\n"
          ]
        }
      ]
    }
  ]
}